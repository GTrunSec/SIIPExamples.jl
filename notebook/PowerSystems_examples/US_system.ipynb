{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a `System` representing the entire U.S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Originally Contributed by**: Clayton Barrows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to assemble a `System` representing the entire U.S. using\n",
    "[PowerSystems.jl](https://github.com/NREL-SIIP/powersystems.jl) and the data assembled by\n",
    "[Xu, et. al.](https://arxiv.org/abs/2002.06155). We'll use the same tabular data parsing\n",
    "capability [demonstrated on the RTS-GMLC dataset](../../notebook/PowerSystems_examples/parse_tabulardata.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "This notebook depends on the SIIPExamples.jl environment which is loaded by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SIIPExamples\n",
    "using PowerSystems\n",
    "using TimeSeries\n",
    "using Dates\n",
    "const PSY = PowerSystems\n",
    "const IS = PSY.InfrastructureSystems;\n",
    "using DataFrames\n",
    "using CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Data\n",
    "PowerSystems.jl links to some test data that is suitable for this example.\n",
    "Let's download the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"downloading data...\"\n",
    "datadir = joinpath(dirname(dirname(pathof(SIIPExamples))), \"US-System\")\n",
    "siip_data = joinpath(datadir, \"SIIP\")\n",
    "if !isdir(datadir)\n",
    "    mkdir(datadir)\n",
    "    mkdir(siip_data)\n",
    "    tempfilename = download(\"https://zenodo.org/record/3735409/files/USATestSystem.zip?download=1\")\n",
    "    SIIPExamples.unzip(SIIPExamples.os, tempfilename, datadir)\n",
    "end\n",
    "\n",
    "config_dir = joinpath(\n",
    "    dirname(dirname(pathof(SIIPExamples))),\n",
    "    \"script\",\n",
    "    \"PowerSystems_examples\",\n",
    "    \"US_config\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Formatting\n",
    "This is a big dataset. Typically one would only want to include one of the interconnects\n",
    "available. Lets use Texas to start. You can set `interconnect = nothing` if you want everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interconnect = \"Texas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few minor incompatibilities between the data and the supported tabular data\n",
    "format. We can resolve those here.\n",
    "\n",
    "First, PowerSystems.jl only supports parsing piecewise linear generator costs from tabular\n",
    "data. So, we can sample the quadratic polynomial cost curves and provide PWL points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"formatting data ...\"\n",
    "!isnothing(interconnect) && @info \"filtering data to include $interconnect ...\"\n",
    "gen = DataFrame(CSV.read(joinpath(datadir, \"plant.csv\")))\n",
    "filter!(row -> row[:interconnect] == interconnect, gen)\n",
    "gencost = CSV.read(joinpath(datadir, \"gencost.csv\"))\n",
    "gen = join(gen, gencost, on = :plant_id, makeunique = true)\n",
    "\n",
    "function make_pwl(gen::DataFrame, traunches = 2)\n",
    "    output_pct_cols = [\"output_percent_\" * string(i) for i in 0:traunches]\n",
    "    hr_cols = [\"heat_rate_incr_\" * string(i) for i in 1:traunches]\n",
    "    pushfirst!(hr_cols, \"heat_rate_avg_0\")\n",
    "    pwl = DataFrame(repeat([Float64], 6), Symbol.(vcat(output_pct_cols, hr_cols)))\n",
    "    for row in eachrow(gen)\n",
    "        traunch_len = (1.0 - row.Pmin / row.Pmax) / traunches\n",
    "        pct = [row.Pmin / row.Pmax + i * traunch_len for i in 0:traunches]\n",
    "        c(pct) = pct * row.Pmax * (row.GenIOB + row.GenIOC^2 + row.GenIOD^3)\n",
    "        hr = [c(pct[1])]\n",
    "        [push!(hr, c(pct[i + 1]) - hr[i]) for i in 1:traunches]\n",
    "        push!(pwl, vcat(pct, hr))\n",
    "    end\n",
    "    return hcat(gen, pwl)\n",
    "end\n",
    "\n",
    "gen = make_pwl(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some incomplete aspects of this dataset. Here, I've assigned some approximate\n",
    "minimum up/down times, ramp rates, and some minor adjustments to categories. There are better\n",
    "and more efficient ways to do this, but this works for this script..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen[:, :unit_type] .= \"OT\"\n",
    "gen[:, :min_up_time] .= 0.0\n",
    "gen[:, :min_down_time] .= 0.0\n",
    "[gen[gen.type .== \"wind\", col] .= [\"Wind\", 0.0, 0.0][ix] for (ix, col) in enumerate([:unit_type, :min_up_time, :min_down_time])]\n",
    "[gen[gen.type .== \"solar\", col] .= [\"PV\", 0.0, 0.0][ix] for (ix, col) in enumerate([:unit_type, :min_up_time, :min_down_time])]\n",
    "[gen[gen.type .== \"hydro\", col] .= [\"HY\", 0.0, 0.0][ix] for (ix, col) in enumerate([:unit_type, :min_up_time, :min_down_time])]\n",
    "[gen[gen.type .== \"ng\", col] .= [4.5, 8, 5][ix] for (ix, col) in enumerate([:min_up_time, :min_down_time, :ramp_30])]\n",
    "[gen[gen.type .== \"coal\", col] .= [24, 48, 4][ix] for (ix, col) in enumerate([:min_up_time, :min_down_time, :ramp_30])]\n",
    "[gen[gen.type .== \"nuclear\", col] .= [72, 72, 2][ix] for (ix, col) in enumerate([:min_up_time, :min_down_time, :ramp_30])]\n",
    "\n",
    "gen[:, :name] = \"gen\" .* string.(gen.plant_id)\n",
    "CSV.write(joinpath(siip_data, \"gen.csv\"), gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also merge the zone.csv with the bus.csv and identify bus types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus = DataFrame(CSV.read(joinpath(datadir, \"bus.csv\")))\n",
    "!isnothing(interconnect) && filter!(row -> row[:interconnect] == interconnect, bus)\n",
    "zone = CSV.read(joinpath(datadir, \"zone.csv\"))\n",
    "bus = join(bus, zone, on = :zone_id, kind = :left)\n",
    "int2bustype(b) = replace(split(string(PSY.BusTypes.BusType(b)), \".\")[end], \"]\" => \"\")\n",
    "bus.bustype = int2bustype.(bus.type)\n",
    "bus.name = \"bus\" .* string.(bus.bus_id)\n",
    "CSV.write(joinpath(siip_data, \"bus.csv\"), bus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need branch names as strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = DataFrame(CSV.read(joinpath(datadir, \"branch.csv\")))\n",
    "!isnothing(interconnect) && filter!(row -> row[:interconnect] == interconnect, branch)\n",
    "branch.name = \"branch\" .* string.(branch.branch_id)\n",
    "branch.ratio = Float64.(branch.ratio)\n",
    "CSV.write(joinpath(siip_data, \"branch.csv\"), branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PowerSystems parser expects the files to be named a certain way.\n",
    "And, we need a \"control_mode\" column in dc-line data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcbranch = DataFrame(CSV.read(joinpath(datadir, \"dcline.csv\")))\n",
    "!isnothing(interconnect) && filter!(row -> row[:from_bus_id] in bus.bus_id, dcbranch)\n",
    "!isnothing(interconnect) && filter!(row -> row[:to_bus_id] in bus.bus_id, dcbranch)\n",
    "dcbranch.name = \"dcbranch\" .* string.(dcbranch.dcline_id)\n",
    "dcbranch[:, :control_mode] .= \"Power\"\n",
    "CSV.write(joinpath(siip_data, \"dc_branch.csv\"), dcbranch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to create a reference for where to get timeseries data for each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = []\n",
    "ts_csv = [\"wind\", \"solar\", \"hydro\", \"demand\"]\n",
    "plant_ids = Symbol.(string.(gen.plant_id))\n",
    "for f in ts_csv\n",
    "    @info \"formatting $f.csv ...\"\n",
    "    csvpath = joinpath(siip_data, f * \".csv\")\n",
    "    csv = DataFrame(CSV.read(joinpath(datadir, f * \".csv\")))\n",
    "    (category, name_prefix, label) =\n",
    "        f == \"demand\" ? (\"Area\", \"\", \"get_maxactivepower\") :\n",
    "        (\"Generator\", \"gen\", \"get_rating\")\n",
    "    if !(:DateTime in names(csv))\n",
    "        DataFrames.rename!(\n",
    "            csv,\n",
    "            (names(csv)[occursin.(\"UTC\", String.(names(csv)))][1] => :DateTime),\n",
    "        )\n",
    "        csv.DateTime = replace.(csv.DateTime, \" \" => \"T\")\n",
    "    end\n",
    "    device_names = f == \"demand\" ? unique(bus.zone_name) : gen.name\n",
    "    for id in names(csv)\n",
    "        colname = id\n",
    "        if f == \"demand\"\n",
    "            if id in Symbol.(zone.zone_id)\n",
    "                colname = Symbol(zone[Symbol.(zone.zone_id) .== id, :zone_name][1])\n",
    "                DataFrames.rename!(csv, (id => colname))\n",
    "            end\n",
    "        else\n",
    "            if id in plant_ids\n",
    "                colname = Symbol(gen[Symbol.(gen.plant_id) .== id, :name][1])\n",
    "                DataFrames.rename!(csv, (id => colname))\n",
    "            end\n",
    "        end\n",
    "        if String(colname) in device_names\n",
    "            sf = maximum(csv[:, colname]) == 0.0 ? 1.0 : \"Max\"\n",
    "            push!(timeseries, Dict(\n",
    "                \"simulation\" => \"DA\",\n",
    "                \"category\" => category,\n",
    "                \"component_name\" =>String(colname),\n",
    "                \"label\" => label,\n",
    "                \"scaling_factor\" => sf,\n",
    "                \"data_file\" => csvpath))\n",
    "        end\n",
    "    end\n",
    "    CSV.write(csvpath, csv)\n",
    "end\n",
    "\n",
    "timeseries_pointers = joinpath(siip_data, \"timeseries_pointers.json\")\n",
    "open(timeseries_pointers, \"w\") do io\n",
    "    PowerSystems.IS.JSON2.write(io, timeseries)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tabular data format relies on a folder containing `*.csv` files and `.yaml` files\n",
    "describing the column names of each file in PowerSystems terms, and the PowerSystems\n",
    "data type that should be created for each generator type. The respective \"us_decriptors.yaml\"\n",
    "and \"US_generator_mapping.yaml\" files have already been tailored to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"parsing csv files...\"\n",
    "rawsys = PSY.PowerSystemTableData(\n",
    "    siip_data,\n",
    "    100.0,\n",
    "    joinpath(config_dir, \"us_descriptors.yaml\"),\n",
    "    generator_mapping_file = joinpath(config_dir, \"US_generator_mapping.yaml\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a `System`\n",
    "Next, we'll create a `System` from the `rawsys` data. Since a `System` is predicated on a\n",
    "forecast resolution and the `rawsys` data includes both 5-minute and 1-hour resolution\n",
    "forecasts, we also need to specify which forecasts we want to include in the `System`.\n",
    "The `forecast_resolution` kwarg filters to only include forecasts with a matching resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"creating System\"\n",
    "sys = System(rawsys; configpath = joinpath(config_dir, \"us_system_validation.json\"));\n",
    "sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all took reasonably long, so we can save our `System` using the serialization\n",
    "capability included with PowerSystems.jl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"serializing System\"\n",
    "io = open(joinpath(siip_data, \"sys.json\"), \"w\")\n",
    "to_json(io, sys)\n",
    "close(io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
